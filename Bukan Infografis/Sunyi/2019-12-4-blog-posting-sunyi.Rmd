---
title: "Sunyi Web Series by Tropicana Slim"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> Data is the new oil

Masih ingat dengan _quotes_ di atas? Di tulisan saya yang [lalu](https://ikanx101.github.io/blog/kuliah-umum-tel-u/), saya pernah menyebutkan bahwa salah satu tantangan bagi kita saat ini terkait data adalah __di mana kita bisa mendapatkannya__. Sejatinya di zaman digital ini, data bertebaran di mana-mana.

Kembali ke judul tulisan ini, __Tropicana Slim__ selalu saja membuat _web series_ yang menarik dan layak ditunggu di _channel_ __Youtube__-nya. Setelah meluncurkan _web series_ __Sore__ dan __Janji__, kini ada __Sunyi__. Hal yang paling serunya lagi adalah __Sunyi__ bertemakan dunia _difabel_ atau _disabilitas_. 

_Cobain deh_, nonton episode [pertamanya](https://www.youtube.com/watch?v=7_0V14TYVzc). Dijamin bikin penasaran (_dan mungkin baper bagi Anda yang jomblo_). _hehe_

Sampai tulisan ini saya buat, __Sunyi__ sudah sampai episode 3 _lhoo_. _Viewers_-nya sudah belasan ribu. 

_Nah_, sekarang saya coba _iseng_ mengambil data dari __Youtube__ terkait _web series_ ini. Untuk melakukannya saya menggunakan bantuan ~~Python~~ __R__ dan __Youtube Data API__. 

Proses membuat _Google Authetication_ -nya mirip dengan layanan _Google Vision AI_, yakni tidak menggunakan __API key__ walau nama layanannya __API__. Bagian ini saya _skip_ yah. Kita langsung ke bagian ambil dan ekstrak datanya. Penasaran data apa saja yang bisa diambil? _Yuk cekidot!_

## Data pertama

Data yang paling mudah diambil adalah data berapa banyak _view_, _like_, _dislike_, _favourite_, dan _comment_. Sepertinya hal yang receh yah. Gak pake bantuan __R__ sebenarnya bisa diambil sendiri _manual_ satu-persatu. Tapi kalau _web series_-nya kayak [Tersanjung](https://id.wikipedia.org/wiki/Tersanjung) _gimana hayo_?

Hasil datanya seperti ini:

```{r,echo=FALSE,message=FALSE}
library(dplyr)
library(ggplot2)
library(wordcloud2)
data = read.csv('stat eps.csv')
```

```{r,echo=FALSE}
data = data %>% mutate(X = NULL,
                id = NULL)
data
```

Heran yah, padahal sudah bagus gitu _web series_-nya, masih ada aja yang _dislike_. _hehe_

Kalau dilihat sekilas, _viewers_-nya menurun tiap episode. Bisa jadi karena memang _web series_-nya baru saja mulai. Jadi belum banyak yang menonton. 

```{r,echo=FALSE}
library(ggthemes)
data %>% ggplot(aes(x=as.factor(episode),y=viewCount)) + geom_line(group=1,color='#fcc603',size=1.2) +
  geom_label(aes(label=viewCount/1000),size=3) + 
  labs(x = 'Episode ke-',y='Total viewers',
       title = 'Viewers: SUNYI dari episode ke episode',
       subtitle = 'source: Youtube DATA API 5 Des 2019 3:56 pm',
       caption = 'https://ikanx101.github.io/') +
  theme_minimal() +
  theme(axis.text.y = element_blank())
```

Walaupun begitu, tren yang berbeda ditunjukkan pada variabel `likeCount` dan `commentCount`. Sempat menurun pada episode kedua tapi naik di episode ketiga.

```{r,echo=FALSE}
library(reshape2)
data %>% select(episode,likeCount,commentCount) %>% melt(id.vars='episode') %>% 
  ggplot(aes(x=as.factor(episode),y=value,group=variable)) + 
  geom_line(aes(color=variable),size=1.2) +
  geom_label(aes(label=value),size=3) +
  labs(x = 'Episode ke-',y='Banyak viewers yang like (merah)\nBanyak viewers yang comment (biru)',
       title = 'Likes dan Comments: SUNYI dari episode ke episode',
       subtitle = 'source: Youtube DATA API 5 Des 2019 3:56 pm',
       caption = 'https://ikanx101.github.io/') +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        legend.position = 'none')
```

Coba yah kalau kita buat variabel `likeCount` dan `commentCount` dalam bentuk rasio terhadap `viewCount`. Bentuknya jadi sebagai berikut:

```{r,echo=FALSE}
# likeCount rasio
data %>% ggplot(aes(x=as.factor(episode),y=likeCount/viewCount)) + geom_line(group=1,color='#d9111b',size=1.2) +
  geom_label(aes(label=round(100*likeCount/viewCount,2)),size=3) + 
  labs(x = 'Episode ke-',y='Rasio likes per total viewers',
       title = 'Rasio Likes: SUNYI dari episode ke episode',
       subtitle = 'source: Youtube DATA API 5 Des 2019 3:56 pm',
       caption = 'https://ikanx101.github.io/') +
  theme_minimal() +
  theme(axis.text.y = element_blank())

# commentCount rasio
data %>% ggplot(aes(x=as.factor(episode),y=commentCount/viewCount)) + geom_line(group=1,color='#0b75b8',size=1.2) +
  geom_label(aes(label=round(100*commentCount/viewCount,2)),size=3) + 
  labs(x = 'Episode ke-',y='Rasio Comments per total viewers',
       title = 'Rasio Comments: SUNYI dari episode ke episode',
       subtitle = 'source: Youtube DATA API 5 Des 2019 3:56 pm',
       caption = 'https://ikanx101.github.io/') +
  theme_minimal() +
  theme(axis.text.y = element_blank())
```

Nah kalau dari rasio sudah terlihat mirip kan yah polanya.

#### Kalau punya data ini trus _So What Gitu Lho?_

Nah, kira-kira ada yang kebayang gak data ini bisa diapakan saja? Nih, saya kasih ide. Satu aja yah tapinya (lainnya harus dipikirkan sendiri donk). _hehe_

> Misal, saya punya data ini (plus data durasi video) dari keseluruhan video yang ada di suatu _channel Youtube_, saya bisa bikin clustering analysis dari semua video itu. Lalu saya bisa analisa, karakteristik dari masing-masing cluster video yang terbentuk. Jangan-jangan ada pola antar beberapa video yang ada. Bisa jadi ide untuk _next content development_-nya. 

Di tulisan berikutnya yah, saya cobain kayak gimana sih analisanya _real_-nya.

## Data kedua

Berikutnya adalah data detail properties dari video tersebut. Mencakup: judul, waktu _upload_, deskripsi, resolusi, _hashtags_ yang digunakan, dll. Contohnya untuk episode 1, data yang dapat dilihat adalah sbb:

```{r,echo=FALSE}
load("/cloud/project/Bukan Infografis/Sunyi/info.rda")
(detail)
```

Saya kurang tertarik dengan data kedua ini, mungkin akan saya _skip_ dulu yah. Kalau teman-teman ada ide untuk melakukan analisa apa, _let me know yah_.

## Data ketiga

Data berikutnya yang bisa diambil adalah data komentar _viewers_ di masing-masing video. Jadi ini data murni komen terhadap videonya yah. Jika ada _reply_ dari komen, datanya ternyata tidak ada. Nah, ini baru menarik. Apa aja isinya? Berikut nama variabelnya yah:

```{r,echo=FALSE,warning=FALSE}
library(readr)
komen <- read_csv("komen.csv", col_types = cols(X1 = col_skip(), 
    authorChannelId.value = col_skip(), canRate = col_skip(), 
    textDisplay  = col_skip(), videoId = col_skip(), 
    viewerRating = col_skip()))
attributes(komen)$spec=NULL
```

```{r}
str(komen)
```
Setidaknya ada delapan variabel yang bisa kita ambil dan analisa, yakni:

1. `authorDisplayName`: nama _commenter_.
2. `authorProfileImageUrl`: _profil pic Youtube account_ dari _commenter_.
3. `authorChannelUrl`: _link channel Youtube_ dari _commenter_.
4. `textOriginal`: komentar.
5. `likeCount`: Berapa banyak yang _likes_ komentar tersebut.
6. `publishedAt`: Kapan pertama kali komentar tersebut _published_.
7. `updatedAt`: Kapan komentar tersebut diedit (jika ada).
8. `episode`: Episode _web series_ sunyi.

Oh iya, bagi rekan-rekan yang mau membedah sendiri datanya, saya lampirkan di [sini](https://raw.githubusercontent.com/ikanx101/belajaR/master/Bukan%20Infografis/Sunyi/komen.csv) yah.

```{r}
head(komen,15)
```

### _Let the fun parts begin!_

Bermodalkan data ketiga, mari kita oprek-oprek data ini. Kita mulai dari yang paling sederhana dulu yah.

#### Komentar paling banyak di- _like_ per episode

```{r,echo=FALSE}
komen$textOriginal = gsub("\\+", "", komen$textOriginal)
komen$textOriginal = gsub("U000.....", "", komen$textOriginal)
komen$textOriginal = gsub("[^[:alnum:][:blank:]?&/\\-]", " ", komen$textOriginal)
komen$textOriginal = gsub('  ',' ',komen$textOriginal)
komen$textOriginal = gsub('  ',' ',komen$textOriginal)
```
```{r}
new = komen %>% group_by(episode) %>% filter(likeCount == max(likeCount)) %>%
  select(episode,authorDisplayName,textOriginal,likeCount) %>% arrange(episode)
new
```

__Pada episode pertama__, komentar yang paling banyak di- _like_ adalah komentar:
```{r,echo=FALSE}
new$textOriginal[1]
```

Dengan `likeCount` sebanyak 18. 

> Sebuah pujian yang tulus dari viewer terkait betapa bagusnya _web series_-nya TropicanaSlim.

__Sedangkan pada episode kedua__, komentar yang paling banyak di- _like_ adalah komentar:
```{r,echo=FALSE}
new$textOriginal[2]
```

Dengan `likeCount` sebanyak 17.

> Nah, ini adalah pertanyaan viewer terkait durasi _web series_ ini yang lebih singkat dibandingkan __Sore__ dan __Janji__.

__Sedangkan pada episode ketiga__, komentar yang paling banyak di- _like_ adalah komentar:
```{r,echo=FALSE}
new$textOriginal[3]
```

Dengan `likeCount` sebanyak 29.

> Ini adalah komen dari akun resmi __Sunyi Coffee__. Sebenarnya, komen ini selalu muncul di setiap episode. Tapi baru di episode ketiga ini yang paling banyak di- _likes_ oleh _viewer_.

#### Siapa yang komen?

Saya sih penasaran, apakah yang komen di ketiga video itu selalu orang yang sama atau beda-beda yah. Yuk kita cek datanya.

```{r,echo=FALSE}
kesimpulan = 
  komen %>% group_by(authorDisplayName) %>% summarise(freq=n()) %>% 
  mutate(summary = case_when(freq>=3 ~ 'Viewers yang komen di 3 video',
                             freq==2 ~ 'Viewers yang komen di 2 video',
                             freq==1 ~ 'Viewers yang komen di 1 video')) %>%
  group_by(summary) %>% summarise(banyak=n())
kesimpulan
```

```{r,echo=FALSE}
library(ggpubr)
kesimpulan %>% ggplot(aes(x=summary,y=banyak)) + geom_col(color='steelblue',fill='white') + 
  geom_label(aes(label=banyak),size=3) + theme_pubclean() +
  labs(y='Berapa banyak viewers',
       title='Berapa banyak viewer yang komen di web Series Sunyi?',
       subtitle = 'source: Youtube DATA API 5 Des 2019 3:56 pm',
       caption = 'https://ikanx101.github.io/') +
  theme(axis.text.y = element_blank(),
        axis.title.x = element_blank())
```

> Ternyata ada juga lho viewers yang selalu komen di 3 video. Selain __Sunyi Coffee__, ada yang bisa kasih tau siapa saja mereka? Dan apa komentarnya?

#### _Text analysis_ dari komentar _viewer_

Ini bagian paling seru. Bagaimana kita bisa melakukan _text analysis_ dari komentar-komentar _viewers_. Setidaknya ada empat analisa yang bisa dilakukan:

1. __Wordcloud__: Menghitung berapa banyak kata keluar dalam suatu data teks (_Word counting_).
2. __Bi-Gram__: Melihat dua kata yang sering bermunculan bersamaan.
3. __Word Association__: Melihat kumpulan dan hubungan kata-kata yang memiliki asosiasi tinggi.
4. __Topic modellling__: mencari tahu kesamaan dari sekian banyak komentar itu dan dikelompokkan berdasarkan kesamaannya tersebut.
5. __Sentiment Analysis__: Menganalisa sentimen dari masing-masing komentar _viewers_. Apakah positif atau negatif?

Oke, kita mulai dari yang paling gampang dulu yah.

##### Wordcloud

Analisa ini sebenarnya sangat mudah. Hanya menghitung ada berapa banyak kata yang keluar dalam suatu data teks. Sebelum mulai, kita akan lakukan _preparation_ terhadap data teksnya. Apa itu? Mengubah tulisannya jadi _lowercase_ biar mudah dianalisa. 

Berhubung ini hanya untuk _fun_ aja, saya akan _skip_ bagian _word stemming_ dan _remove stopwords_.

> _Kindly Googling about those two terms yah in parts of text analytics_

_Yuk_ kita lihat kata-kata apa saja yang sering muncul!

Berikut adalah semua kata yang punya frekuensi minimal 5 yah:

```{r,echo=FALSE}
komen$textOriginal = tolower(komen$textOriginal)

library(tidytext)
tes = komen %>% mutate(id = c(1:length(textOriginal))) %>%
  select(id,textOriginal) %>%
  unnest_tokens('words',textOriginal) %>%
  count(words,sort=T) %>% filter(n>=5)
tes
```

_Wordcloud_-nya seperti ini yah:

![alt text](https://raw.githubusercontent.com/ikanx101/belajaR/master/Bukan%20Infografis/Sunyi/sunyi%20wordcloud.png "tes")

> _Gimana, ada informasi yang bisa diambil dari wordcloud di atas?_

Ada yang mau _ngerjain_ _wordcloud_ per episodenya?

##### Bi-Gram

Bi-gram adalah pasangan kata yang selalu muncul secara bersamaan dan berurutan. Nah, kita akan lihat, apakah ada pasangan kata yang sering muncul pada komentar _viewers_?

Berikut adalah grafik dari _bigrams_ yang memiliki frekuensi kemunculan lebih dari dua (`>2`):

```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(igraph)
library(ggraph)
library(tidyr)
komen %>% mutate(id = c(1:length(textOriginal))) %>%
  select(id,textOriginal) %>%
  unnest_tokens(bigram,textOriginal,token='ngrams',n=2) %>%
  count(bigram,sort=T) %>% filter(!is.na(bigram)) %>% filter(n>2) %>%
  separate(bigram,into=c('word1','word2'),sep=' ') %>%
  graph_from_data_frame() %>%
  ggraph(layout = 'fr') +
  theme_pubclean() +
  geom_edge_link(aes(edge_alpha=n),
                 show.legend = F,
                 color='darkred') +
  geom_node_point(size=1,color='steelblue') +
  geom_node_text(aes(label=name),alpha=0.4,size=3,vjust=1,hjust=1) +
  theme_void()
```

##### Word Association

_Word association_ itu sesimpel mencari kata yang berkorelasi dengan kata lain. Bagaimana caranya? Kita gunakan _library_ `NLP` dan `tm`.

```{r,message=FALSE,echo=FALSE}
library(NLP)
library(tm)
library(igraph)
library(ggraph)

NAME = Corpus(VectorSource(komen$textOriginal))
NAME = tm_map(NAME, content_transformer(tolower))
NAME = tm_map(NAME,removePunctuation)
NAME = tm_map(NAME, stripWhitespace)
NAME = tm_map(NAME,removeWords, stopwords("en"))
NAME = tm_map(NAME,removeNumbers)

dtm <- DocumentTermMatrix(NAME)
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm<- dtm[rowTotals> 0, ]           #remove all docs without words
cor_g <- cor(as.matrix(dtm))
cor_g <- graph_from_adjacency_matrix(cor_g, mode='undirected', weighted = 'correlation')
cor_edge_list <- as_data_frame(cor_g, 'edges')
only_sig <- cor_edge_list[abs(cor_edge_list$correlation) > 1, ]

colnames(only_sig) = c('word1','word2','n')

only_sig$sama = ifelse(only_sig$word1 == only_sig$word2,1,0)
only_sig = only_sig %>% filter(sama!=1)
```

```{r}
only_sig %>% graph_from_data_frame() %>%
  ggraph(layout = 'fr') +
  theme_pubclean() +
  geom_edge_link(show.legend = F,
                 color='darkred') +
  geom_node_point(size=1,color='steelblue') +
  geom_node_text(aes(label=name),alpha=0.4,size=1.25,vjust=1,hjust=1) +
  theme_void()
```


